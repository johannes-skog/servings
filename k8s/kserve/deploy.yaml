apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: test
spec:
  predictor:
    minReplicas: 1
    timeout: 60
    batcher:
      maxBatchSize: 32
      maxLatency: 500
    serviceAccountName: sa
    triton:
      args:
      - --log-verbose=1
      storageUri: https://deeplearning.blob.core.windows.net/kfserve2/models/
      runtimeVersion: 20.10-py3
      env:
      - name: OMP_NUM_THREADS
        value: "1"
      resources:
        limits:
          cpu: "4"
          memory: "4Gi"
        requests:
          cpu: "1"
          memory: "1Gi"
    